{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6758079,"sourceType":"datasetVersion","datasetId":3890129},{"sourceId":19764,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":16394}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\n\nDATA_DIR = \"/kaggle/input/vqa-rad-visual-question-answering-radiology\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-01T13:45:17.114241Z","iopub.execute_input":"2024-04-01T13:45:17.114852Z","iopub.status.idle":"2024-04-01T13:45:17.464769Z","shell.execute_reply.started":"2024-04-01T13:45:17.114809Z","shell.execute_reply":"2024-04-01T13:45:17.464045Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel(DATA_DIR+\"/VQA_RAD Dataset Public.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:18.539317Z","iopub.execute_input":"2024-04-01T13:45:18.540262Z","iopub.status.idle":"2024-04-01T13:45:19.570976Z","shell.execute_reply.started":"2024-04-01T13:45:18.540228Z","shell.execute_reply":"2024-04-01T13:45:19.569979Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n  warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:20.443346Z","iopub.execute_input":"2024-04-01T13:45:20.443780Z","iopub.status.idle":"2024-04-01T13:45:20.466844Z","shell.execute_reply.started":"2024-04-01T13:45:20.443755Z","shell.execute_reply":"2024-04-01T13:45:20.465786Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   QID_unique  QID_para                            QID_linked  \\\n0           0  freeform  03f451ca-de62-4617-9679-e836026a7642   \n1           1  freeform  06e26b2c-04b9-42bc-8e98-1de30a0f7682   \n2           2  freeform  0d0e8b6b-7753-4788-9b6d-dc7f25250c3f   \n3           3  freeform  0e90b6bc-265f-490b-a039-509b9907a3cb   \n4           4  freeform  1179f612-12e0-4dda-aee0-f14a5200be7b   \n\n                                        IMAGEID_case  \\\n0  https://medpix.nlm.nih.gov/case?id=48e1dd0e-85...   \n1  https://medpix.nlm.nih.gov/case?id=b197277b-69...   \n2  https://medpix.nlm.nih.gov/case?id=b197277b-69...   \n3  https://medpix.nlm.nih.gov/case?id=19aa8a2b-35...   \n4  https://medpix.nlm.nih.gov/case?id=b197277b-69...   \n\n                                             IMAGEID IMAGEORGAN  \\\n0  https://medpix.nlm.nih.gov/images/full/synpic5...       HEAD   \n1  https://medpix.nlm.nih.gov/images/full/synpic2...      CHEST   \n2  https://medpix.nlm.nih.gov/images/full/synpic2...      CHEST   \n3  https://medpix.nlm.nih.gov/images/full/synpic2...      CHEST   \n4  https://medpix.nlm.nih.gov/images/full/synpic2...      CHEST   \n\n      EVALUATION                                       QUESTION Q_REPHASE  \\\n0  not evaluated            Are regions of the brain infarcted?       NaN   \n1  not evaluated                Are the lungs normal appearing?       NaN   \n2  not evaluated            Is there evidence of a pneumothorax       NaN   \n3          given  What type of imaging does this not represent?       NaN   \n4          given                    Is this a MRI of the chest?       NaN   \n\n  Q_RELATION Q_FRAMED    Q_TYPE      ANSWER  A_TYPE  \n0        NaN      NaN      PRES         Yes  CLOSED  \n1        NaN      NaN       ABN          No  CLOSED  \n2        NaN      NaN      PRES          No  CLOSED  \n3        NaN      NaN  MODALITY  ultrasound    OPEN  \n4        NaN      NaN  MODALITY          no  CLOSED  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QID_unique</th>\n      <th>QID_para</th>\n      <th>QID_linked</th>\n      <th>IMAGEID_case</th>\n      <th>IMAGEID</th>\n      <th>IMAGEORGAN</th>\n      <th>EVALUATION</th>\n      <th>QUESTION</th>\n      <th>Q_REPHASE</th>\n      <th>Q_RELATION</th>\n      <th>Q_FRAMED</th>\n      <th>Q_TYPE</th>\n      <th>ANSWER</th>\n      <th>A_TYPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>freeform</td>\n      <td>03f451ca-de62-4617-9679-e836026a7642</td>\n      <td>https://medpix.nlm.nih.gov/case?id=48e1dd0e-85...</td>\n      <td>https://medpix.nlm.nih.gov/images/full/synpic5...</td>\n      <td>HEAD</td>\n      <td>not evaluated</td>\n      <td>Are regions of the brain infarcted?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PRES</td>\n      <td>Yes</td>\n      <td>CLOSED</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>freeform</td>\n      <td>06e26b2c-04b9-42bc-8e98-1de30a0f7682</td>\n      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n      <td>https://medpix.nlm.nih.gov/images/full/synpic2...</td>\n      <td>CHEST</td>\n      <td>not evaluated</td>\n      <td>Are the lungs normal appearing?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ABN</td>\n      <td>No</td>\n      <td>CLOSED</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>freeform</td>\n      <td>0d0e8b6b-7753-4788-9b6d-dc7f25250c3f</td>\n      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n      <td>https://medpix.nlm.nih.gov/images/full/synpic2...</td>\n      <td>CHEST</td>\n      <td>not evaluated</td>\n      <td>Is there evidence of a pneumothorax</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PRES</td>\n      <td>No</td>\n      <td>CLOSED</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>freeform</td>\n      <td>0e90b6bc-265f-490b-a039-509b9907a3cb</td>\n      <td>https://medpix.nlm.nih.gov/case?id=19aa8a2b-35...</td>\n      <td>https://medpix.nlm.nih.gov/images/full/synpic2...</td>\n      <td>CHEST</td>\n      <td>given</td>\n      <td>What type of imaging does this not represent?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MODALITY</td>\n      <td>ultrasound</td>\n      <td>OPEN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>freeform</td>\n      <td>1179f612-12e0-4dda-aee0-f14a5200be7b</td>\n      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n      <td>https://medpix.nlm.nih.gov/images/full/synpic2...</td>\n      <td>CHEST</td>\n      <td>given</td>\n      <td>Is this a MRI of the chest?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MODALITY</td>\n      <td>no</td>\n      <td>CLOSED</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Function to extract filename and append to parent dir\n\ndef extract_filename_and_append(row, parent_dir):\n    filename = os.path.basename(row['IMAGEID'])\n    return os.path.join(parent_dir, filename)\n\ndata['img_path'] = data.apply(extract_filename_and_append, parent_dir=DATA_DIR+\"/VQA_RAD Image Folder/VQA_RAD Image Folder\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:21.381007Z","iopub.execute_input":"2024-04-01T13:45:21.381888Z","iopub.status.idle":"2024-04-01T13:45:21.423631Z","shell.execute_reply.started":"2024-04-01T13:45:21.381853Z","shell.execute_reply":"2024-04-01T13:45:21.422936Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data[\"img_path\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:22.757455Z","iopub.execute_input":"2024-04-01T13:45:22.758113Z","iopub.status.idle":"2024-04-01T13:45:22.763979Z","shell.execute_reply.started":"2024-04-01T13:45:22.758083Z","shell.execute_reply":"2024-04-01T13:45:22.763068Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/vqa-rad-visual-question-answering-radiology/VQA_RAD Image Folder/VQA_RAD Image Folder/synpic54610.jpg'"},"metadata":{}}]},{"cell_type":"code","source":"data['cleaned_question'] = data['QUESTION'].str.replace(\"?\", \"\").str.lower()\ndata['labels'] = data['ANSWER'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:27.487863Z","iopub.execute_input":"2024-04-01T13:45:27.488267Z","iopub.status.idle":"2024-04-01T13:45:27.501322Z","shell.execute_reply.started":"2024-04-01T13:45:27.488237Z","shell.execute_reply":"2024-04-01T13:45:27.500145Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data['labels'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:28.097871Z","iopub.execute_input":"2024-04-01T13:45:28.098227Z","iopub.status.idle":"2024-04-01T13:45:28.111513Z","shell.execute_reply.started":"2024-04-01T13:45:28.098202Z","shell.execute_reply":"2024-04-01T13:45:28.110618Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"labels\nno                        606\nyes                       587\naxial                      43\nright                      26\nleft                       19\n                         ... \nvasculature                 1\nmri diffusion weighted      1\ninfarcts                    1\nembolus                     1\nirregular                   1\nName: count, Length: 512, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:29.168626Z","iopub.execute_input":"2024-04-01T13:45:29.169378Z","iopub.status.idle":"2024-04-01T13:45:29.175985Z","shell.execute_reply.started":"2024-04-01T13:45:29.169338Z","shell.execute_reply":"2024-04-01T13:45:29.174962Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(2248, 17)"},"metadata":{}}]},{"cell_type":"code","source":"# Identify classes with less than 200 samples\nclasses_to_augment = data['labels'].value_counts()[data['labels'].value_counts() < 200]\nprint(classes_to_augment)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:30.116856Z","iopub.execute_input":"2024-04-01T13:45:30.117310Z","iopub.status.idle":"2024-04-01T13:45:30.129279Z","shell.execute_reply.started":"2024-04-01T13:45:30.117282Z","shell.execute_reply":"2024-04-01T13:45:30.128416Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"labels\naxial                     43\nright                     26\nleft                      19\npa                        15\nct                        13\n                          ..\nvasculature                1\nmri diffusion weighted     1\ninfarcts                   1\nembolus                    1\nirregular                  1\nName: count, Length: 510, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# torch vae","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:32.438721Z","iopub.execute_input":"2024-04-01T13:45:32.439404Z","iopub.status.idle":"2024-04-01T13:45:38.695192Z","shell.execute_reply.started":"2024-04-01T13:45:32.439375Z","shell.execute_reply":"2024-04-01T13:45:38.694377Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Sampling(nn.Module):\n    def forward(self, z_mean, z_log_var):\n        batch, dim = z_mean.size()\n        epsilon = torch.randn_like(z_mean)\n        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(256 * 4 * 4, 256)  # Corrected for 4x4 spatial size at the bottleneck\n        self.fc_mean = nn.Linear(256, latent_dim)\n        self.fc_log_var = nn.Linear(256, latent_dim)\n        self.sampling = Sampling()\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = self.flatten(x)\n        x = F.relu(self.fc(x))\n        z_mean = self.fc_mean(x)\n        z_log_var = self.fc_log_var(x)\n        z = self.sampling(z_mean, z_log_var)\n        return z_mean, z_log_var, z\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Decoder, self).__init__()\n        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)  # Adjusted to match encoder output\n        self.conv_transpose1 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1)\n        self.conv_transpose2 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)\n        self.conv_transpose3 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n        self.conv_transpose4 = nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1)\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(-1, 256, 4, 4)  # Correct view for the decoder input\n        x = F.relu(self.conv_transpose1(x))\n        x = F.relu(self.conv_transpose2(x))\n        x = F.relu(self.conv_transpose3(x))\n        x = torch.sigmoid(self.conv_transpose4(x))  # Use sigmoid for final layer to bound the output\n        return x\n\nclass VAE(nn.Module):\n    def __init__(self, latent_dim):\n        super(VAE, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def forward(self, x):\n        z_mean, z_log_var, z = self.encoder(x)\n        z = self.encoder.sampling(z_mean, z_log_var)\n        reconstruction = self.decoder(z)\n        return reconstruction, z_mean, z_log_var\n\ndef vae_loss(reconstruction, x, z_mean, z_log_var):\n    reconstruction_loss = F.mse_loss(reconstruction, x, reduction='sum')\n    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n    return reconstruction_loss + kl_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:38.696745Z","iopub.execute_input":"2024-04-01T13:45:38.697328Z","iopub.status.idle":"2024-04-01T13:45:38.716251Z","shell.execute_reply.started":"2024-04-01T13:45:38.697302Z","shell.execute_reply":"2024-04-01T13:45:38.715275Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, img_paths, transform=None):\n        self.img_paths = img_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = CustomDataset(data['img_path'].tolist(), transform=transform)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:45:38.717437Z","iopub.execute_input":"2024-04-01T13:45:38.717728Z","iopub.status.idle":"2024-04-01T13:45:38.735067Z","shell.execute_reply.started":"2024-04-01T13:45:38.717693Z","shell.execute_reply":"2024-04-01T13:45:38.734315Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Set device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model and optimizer\nlatent_dim = 128  # Define latent dimension size\nvae = VAE(latent_dim=latent_dim).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=1e-3)\n\n# Training parameters\nepochs = 50  # Number of epochs to train for\n\n# Lists to store losses for plotting or analysis\nepoch_losses = []\n\n# Training loop\nfor epoch in range(epochs):\n    epoch_loss = 0  # Reset epoch loss at the start of each epoch\n    vae.train()  # Set model to training mode\n    \n    with tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\") as batch_bar:\n        for batch_idx, images in batch_bar:\n            images = images.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass through VAE\n            reconstruction, z_mean, z_log_var = vae(images)\n            loss = vae_loss(reconstruction, images, z_mean, z_log_var)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n\n            # Update the progress bar\n            batch_bar.set_postfix(loss=f\"{loss.item():.4f}\", epoch_loss=f\"{epoch_loss/(batch_idx+1):.4f}\")\n\n    # Append the average loss for the current epoch\n    epoch_losses.append(epoch_loss / len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-02-05T19:24:50.184103Z","iopub.execute_input":"2024-02-05T19:24:50.184478Z","iopub.status.idle":"2024-02-05T19:39:37.353660Z","shell.execute_reply.started":"2024-02-05T19:24:50.184445Z","shell.execute_reply":"2024-02-05T19:39:37.352634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_save_path = 'vae_model.pth'\n# torch.save(vae.state_dict(), model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:45:36.018717Z","iopub.execute_input":"2024-02-03T17:45:36.019085Z","iopub.status.idle":"2024-02-03T17:45:36.023486Z","shell.execute_reply.started":"2024-02-03T17:45:36.019056Z","shell.execute_reply":"2024-02-03T17:45:36.022236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:45:36.967605Z","iopub.execute_input":"2024-02-03T17:45:36.967973Z","iopub.status.idle":"2024-02-03T17:45:36.973184Z","shell.execute_reply.started":"2024-02-03T17:45:36.967944Z","shell.execute_reply":"2024-02-03T17:45:36.972186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndef generate_and_plot_image(image_path, vae_model):\n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Move to the same device as model\n    image_tensor = image_tensor.to(next(vae_model.parameters()).device)\n    \n    # Generate the image\n    with torch.no_grad():\n        reconstructed, _, _ = vae_model(image_tensor)\n    \n    # Denormalize the images\n    def denormalize(tensors):\n        return tensors * 0.5 + 0.5\n\n    # Convert tensors to numpy arrays for plotting\n    original = denormalize(image_tensor.squeeze()).permute(1, 2, 0).cpu().numpy()\n    generated = denormalize(reconstructed.squeeze()).permute(1, 2, 0).cpu().numpy()\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    axes[1].imshow(generated)\n    axes[1].set_title('Generated Image')\n    axes[1].axis('off')\n\n    plt.show()\n\n# Example usage:\n# Assuming `vae` is your trained VAE model and `image_path` is the path to your input image\ngenerate_and_plot_image(data[\"img_path\"][1], vae)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:45:41.973387Z","iopub.execute_input":"2024-02-03T17:45:41.974039Z","iopub.status.idle":"2024-02-03T17:45:42.283616Z","shell.execute_reply.started":"2024-02-03T17:45:41.974008Z","shell.execute_reply":"2024-02-03T17:45:42.282689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndef generate_and_plot_image(image_path, vae_model):\n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Move to the same device as model\n    image_tensor = image_tensor.to(next(vae_model.parameters()).device)\n    \n    # Generate the image\n    with torch.no_grad():\n        reconstructed, _, _ = vae_model(image_tensor)\n    \n    # Denormalize the images\n    def denormalize(tensors):\n        return tensors * 0.5 + 0.5\n\n    # Convert tensors to numpy arrays for plotting\n    original = denormalize(image_tensor.squeeze()).permute(1, 2, 0).cpu().numpy()\n    generated = denormalize(reconstructed.squeeze()).permute(1, 2, 0).cpu().numpy()\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    axes[1].imshow(generated)\n    axes[1].set_title('Generated Image')\n    axes[1].axis('off')\n\n    plt.show()\n\n# Example usage:\n# Assuming `vae` is your trained VAE model and `image_path` is the path to your input image\ngenerate_and_plot_image(data[\"img_path\"][700], vae)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:48:55.535947Z","iopub.execute_input":"2024-02-03T16:48:55.536570Z","iopub.status.idle":"2024-02-03T16:48:55.810080Z","shell.execute_reply.started":"2024-02-03T16:48:55.536538Z","shell.execute_reply":"2024-02-03T16:48:55.808808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Plot","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla vae","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom typing import List\nfrom torch import Tensor\n\nclass VanillaVAE(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 latent_dim: int,\n                 hidden_dims: List = None,\n                 **kwargs) -> None:\n        super(VanillaVAE, self).__init__()\n\n        self.latent_dim = latent_dim\n\n        modules = []\n        if hidden_dims is None:\n            hidden_dims = [32, 64, 128, 256, 512]\n\n        # Build Encoder\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size= 3, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        self.encoder = nn.Sequential(*modules)\n        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n\n\n        # Build Decoder\n        modules = []\n\n        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n\n        hidden_dims.reverse()\n\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=3,\n                                       stride = 2,\n                                       padding=1,\n                                       output_padding=1),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU())\n            )\n\n\n\n        self.decoder = nn.Sequential(*modules)\n\n        self.final_layer = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=2,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n                                      kernel_size= 3, padding= 1),\n                            nn.Tanh())\n\n    def encode(self, input: Tensor) -> List[Tensor]:\n        \"\"\"\n        Encodes the input by passing through the encoder network\n        and returns the latent codes.\n        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n        :return: (Tensor) List of latent codes\n        \"\"\"\n        result = self.encoder(input)\n        result = torch.flatten(result, start_dim=1)\n\n        # Split the result into mu and var components\n        # of the latent Gaussian distribution\n        mu = self.fc_mu(result)\n        log_var = self.fc_var(result)\n\n        return [mu, log_var]\n\n    def decode(self, z: Tensor) -> Tensor:\n        \"\"\"\n        Maps the given latent codes\n        onto the image space.\n        :param z: (Tensor) [B x D]\n        :return: (Tensor) [B x C x H x W]\n        \"\"\"\n        result = self.decoder_input(z)\n        result = result.view(-1, 512, 2, 2)\n        result = self.decoder(result)\n        result = self.final_layer(result)\n        return result\n\n    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n        \"\"\"\n        Reparameterization trick to sample from N(mu, var) from\n        N(0,1).\n        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n        :return: (Tensor) [B x D]\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return eps * std + mu\n\n    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n        mu, log_var = self.encode(input)\n        z = self.reparameterize(mu, log_var)\n        return  [self.decode(z), input, mu, log_var]\n\n    def loss_function(self,\n                      *args,\n                      **kwargs) -> dict:\n        \"\"\"\n        Computes the VAE loss function.\n        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n        :param args:\n        :param kwargs:\n        :return:\n        \"\"\"\n        recons = args[0]\n        input = args[1]\n        mu = args[2]\n        log_var = args[3]\n\n        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n        recons_loss =F.mse_loss(recons, input)\n\n\n        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n\n        loss = recons_loss + kld_weight * kld_loss\n        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n\n    def sample(self,\n               num_samples:int,\n               current_device: int, **kwargs) -> Tensor:\n        \"\"\"\n        Samples from the latent space and return the corresponding\n        image space map.\n        :param num_samples: (Int) Number of samples\n        :param current_device: (Int) Device to run the model\n        :return: (Tensor)\n        \"\"\"\n        z = torch.randn(num_samples,\n                        self.latent_dim)\n\n        z = z.to(current_device)\n\n        samples = self.decode(z)\n        return samples\n\n    def generate(self, x: Tensor, **kwargs) -> Tensor:\n        \"\"\"\n        Given an input image x, returns the reconstructed image\n        :param x: (Tensor) [B x C x H x W]\n        :return: (Tensor) [B x C x H x W]\n        \"\"\"\n\n        return self.forward(x)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:53:53.603061Z","iopub.execute_input":"2024-04-01T13:53:53.603788Z","iopub.status.idle":"2024-04-01T13:53:53.627499Z","shell.execute_reply.started":"2024-04-01T13:53:53.603758Z","shell.execute_reply":"2024-04-01T13:53:53.626579Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, img_paths, transform=None):\n        self.img_paths = img_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = CustomDataset(data['img_path'].tolist(), transform=transform)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:53:55.199497Z","iopub.execute_input":"2024-04-01T13:53:55.199855Z","iopub.status.idle":"2024-04-01T13:53:55.208177Z","shell.execute_reply.started":"2024-04-01T13:53:55.199827Z","shell.execute_reply":"2024-04-01T13:53:55.207298Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_params = {\n    \"in_channels\": 3,\n    \"latent_dim\": 128,\n    \"train_batch_size\": 64,\n    \"LR\": 0.0005,\n    \"weight_decay\": 0.0,\n    \"scheduler_gamma\": 0.95,\n    \"kld_weight\": 0.00025,   \n    \"max_epochs\": 100\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:53:56.122872Z","iopub.execute_input":"2024-04-01T13:53:56.123703Z","iopub.status.idle":"2024-04-01T13:53:56.128204Z","shell.execute_reply.started":"2024-04-01T13:53:56.123671Z","shell.execute_reply":"2024-04-01T13:53:56.127236Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from torch import optim\nfrom tqdm import tqdm\n\n# Initialize the model\nvae = VanillaVAE(in_channels=model_params[\"in_channels\"], \n                 latent_dim=model_params[\"latent_dim\"])\n\n# Assuming the device is set appropriately\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvae.to(device)\n\n# Optimizer and Scheduler\noptimizer = optim.Adam(vae.parameters(), lr=model_params[\"LR\"], weight_decay=model_params[\"weight_decay\"])\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=model_params[\"scheduler_gamma\"])\n\n# Lists to store losses for plotting\nplot_loss = []\nplot_rloss = []\nplot_kldloss = []\n\n# Training loop\nfor epoch in range(model_params['max_epochs']):\n    vae.train()\n    epoch_loss = 0\n    epoch_rloss = 0\n    epoch_kldloss = 0\n    with tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{model_params['max_epochs']}\") as batch_bar:\n        for batch_idx, images in batch_bar:\n            images = images.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            recons, _, mu, log_var = vae(images)\n            train_loss = vae.loss_function(recons, images, mu, log_var, M_N=model_params['kld_weight'])\n\n            # Backward pass and optimize\n            train_loss['loss'].backward()\n            optimizer.step()\n            \n            epoch_loss += train_loss['loss'].item()\n            epoch_rloss += train_loss['Reconstruction_Loss'].item()\n            epoch_kldloss += train_loss['KLD'].item()\n            \n            # Update the progress bar\n            batch_bar.set_postfix(loss=f\"{train_loss['loss'].item():.4f}\", \n                                  epoch_loss=f\"{epoch_loss/(batch_idx+1):.4f}\",\n                                  recon_loss=f\"{epoch_rloss/(batch_idx+1):.4f}\",\n                                  kld_loss=f\"{epoch_kldloss/(batch_idx+1):.4f}\")\n        \n        # Append the average losses for the epoch\n        plot_loss.append(epoch_loss / len(dataloader))\n        plot_rloss.append(epoch_rloss / len(dataloader))\n        plot_kldloss.append(epoch_kldloss / len(dataloader))\n\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:36:54.036370Z","iopub.execute_input":"2024-02-08T18:36:54.036745Z","iopub.status.idle":"2024-02-08T19:03:28.761838Z","shell.execute_reply.started":"2024-02-08T18:36:54.036715Z","shell.execute_reply":"2024-02-08T19:03:28.760760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_path = 'vae_model.pth'\ntorch.save(vae.state_dict(), model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T19:36:50.823373Z","iopub.execute_input":"2024-02-08T19:36:50.824127Z","iopub.status.idle":"2024-02-08T19:36:50.869588Z","shell.execute_reply.started":"2024-02-08T19:36:50.824087Z","shell.execute_reply":"2024-02-08T19:36:50.868857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae = VanillaVAE(in_channels=model_params[\"in_channels\"], \n                 latent_dim=model_params[\"latent_dim\"])\n\nvae.load_state_dict(torch.load('/kaggle/input/vae_model/pytorch/1/1/vae_model.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-04-01T13:54:00.435004Z","iopub.execute_input":"2024-04-01T13:54:00.435358Z","iopub.status.idle":"2024-04-01T13:54:01.003589Z","shell.execute_reply.started":"2024-04-01T13:54:00.435331Z","shell.execute_reply":"2024-04-01T13:54:01.002636Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndef denormalize(tensor):\n    # Assuming the same normalization was applied (mean=0.5, std=0.5 for each channel)\n    return tensor * 0.5 + 0.5\n\ndef generate_and_plot_image(image_path, model):\n    # Ensure model is in evaluation mode\n    model.eval()\n    \n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Move tensor to the same device as the model\n    device = next(model.parameters()).device\n    image_tensor = image_tensor.to(device)\n    \n    # Generate reconstructed image\n    with torch.no_grad():\n        reconstructed, _, _, _ = model(image_tensor)\n    \n    # Move tensors back to CPU for plotting\n    image_tensor = image_tensor.to('cpu')\n    reconstructed = reconstructed.to('cpu')\n    \n    # Denormalize images\n    original_img = denormalize(image_tensor.squeeze()).permute(1, 2, 0).numpy()\n    reconstructed_img = denormalize(reconstructed.squeeze()).permute(1, 2, 0).numpy()\n    \n    # Plotting\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(original_img)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    \n    axs[1].imshow(reconstructed_img)\n    axs[1].set_title('Reconstructed Image')\n    axs[1].axis('off')\n#     plt.savefig('original_and_reconstructed.png', bbox_inches='tight')\n    plt.show()\n    \n    plt.imsave(\"original_img.png\", original_img)\n    plt.imsave(\"reconstructed_img.png\", reconstructed_img)\n#     plt.savefig(\"aug.png\")\n\n# Example usage:\n# Assuming `vae` is your trained VAE model and `image_path` is the path to your input image\ngenerate_and_plot_image(data[\"img_path\"][1247], vae)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T14:24:00.014266Z","iopub.execute_input":"2024-04-01T14:24:00.015182Z","iopub.status.idle":"2024-04-01T14:24:00.219677Z","shell.execute_reply.started":"2024-04-01T14:24:00.015148Z","shell.execute_reply":"2024-04-01T14:24:00.218927Z"},"trusted":true},"execution_count":110,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndef denormalize(tensor):\n    # Assuming the same normalization was applied (mean=0.5, std=0.5 for each channel)\n    return tensor * 0.5 + 0.5\n\ndef generate_and_plot_image(image_path, model):\n    # Ensure model is in evaluation mode\n    model.eval()\n    \n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Move tensor to the same device as the model\n    device = next(model.parameters()).device\n    image_tensor = image_tensor.to(device)\n    \n    # Generate reconstructed image\n    with torch.no_grad():\n        reconstructed, _, _, _ = model(image_tensor)\n    \n    # Move tensors back to CPU for plotting\n    image_tensor = image_tensor.to('cpu')\n    reconstructed = reconstructed.to('cpu')\n    \n    # Denormalize images\n    original_img = denormalize(image_tensor.squeeze()).permute(1, 2, 0).numpy()\n    reconstructed_img = denormalize(reconstructed.squeeze()).permute(1, 2, 0).numpy()\n    \n    # Plotting\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(original_img)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    \n    axs[1].imshow(reconstructed_img)\n    axs[1].set_title('Reconstructed Image')\n    axs[1].axis('off')\n    \n    plt.show()\n\n# Example usage:\n# Assuming `vae` is your trained VAE model and `image_path` is the path to your input image\ngenerate_and_plot_image(data[\"img_path\"][600], vae)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T19:59:09.666201Z","iopub.execute_input":"2024-02-08T19:59:09.666606Z","iopub.status.idle":"2024-02-08T19:59:09.877798Z","shell.execute_reply.started":"2024-02-08T19:59:09.666574Z","shell.execute_reply":"2024-02-08T19:59:09.876858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(plot_loss)\nprint(plot_rloss)\nprint(plot_kldloss)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T19:37:08.913371Z","iopub.execute_input":"2024-02-08T19:37:08.913737Z","iopub.status.idle":"2024-02-08T19:37:08.918847Z","shell.execute_reply.started":"2024-02-08T19:37:08.913707Z","shell.execute_reply":"2024-02-08T19:37:08.917953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming plot_loss, plot_rloss, and plot_kldloss are lists containing the loss values for each epoch\n\nplt.figure(figsize=(10, 6))\nplt.plot(plot_loss, label='Total Loss')\nplt.title('VAE Training Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T20:01:46.338591Z","iopub.execute_input":"2024-02-08T20:01:46.339435Z","iopub.status.idle":"2024-02-08T20:01:46.573832Z","shell.execute_reply.started":"2024-02-08T20:01:46.339402Z","shell.execute_reply":"2024-02-08T20:01:46.572978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(plot_rloss, label='Reconstruction Loss', linestyle='--')\nplt.plot(plot_kldloss, label='KL Divergence Loss', linestyle='-.')\nplt.title('VAE Reconstruction and KL Divergence Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T20:02:01.965176Z","iopub.execute_input":"2024-02-08T20:02:01.965885Z","iopub.status.idle":"2024-02-08T20:02:02.189174Z","shell.execute_reply.started":"2024-02-08T20:02:01.965851Z","shell.execute_reply":"2024-02-08T20:02:02.188271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(plot_rloss, label='Reconstruction Loss')\n# plt.plot(plot_kldloss, label='KL Divergence Loss', linestyle='-.')\nplt.title('VAE Reconstruction Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T20:03:15.710500Z","iopub.execute_input":"2024-02-08T20:03:15.711452Z","iopub.status.idle":"2024-02-08T20:03:15.942925Z","shell.execute_reply.started":"2024-02-08T20:03:15.711415Z","shell.execute_reply":"2024-02-08T20:03:15.942059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n# plt.plot(plot_rloss, label='Reconstruction Loss', linestyle='--')\nplt.plot(plot_kldloss, label='KL Divergence Loss', color='orange')\nplt.title('VAE KL Divergence Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T20:03:57.022872Z","iopub.execute_input":"2024-02-08T20:03:57.023781Z","iopub.status.idle":"2024-02-08T20:03:57.248367Z","shell.execute_reply.started":"2024-02-08T20:03:57.023739Z","shell.execute_reply":"2024-02-08T20:03:57.247526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ResNet-VAE","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\nfrom torchvision import models\n\nclass ResNet_VAE(nn.Module):\n    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n        super(ResNet_VAE, self).__init__()\n\n        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n\n        # CNN architechtures\n        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n\n        # encoding components\n        resnet = models.resnet152(pretrained=True)\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n        self.resnet = nn.Sequential(*modules)\n        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n        # Latent vectors mu and sigma\n        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n\n        # Sampling vector\n        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Decoder\n        self.convTrans6 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n                               padding=self.pd4),\n            nn.BatchNorm2d(32, momentum=0.01),\n            nn.ReLU(inplace=True),\n        )\n        self.convTrans7 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n                               padding=self.pd3),\n            nn.BatchNorm2d(8, momentum=0.01),\n            nn.ReLU(inplace=True),\n        )\n\n        self.convTrans8 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n                               padding=self.pd2),\n            nn.BatchNorm2d(3, momentum=0.01),\n            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n        )\n\n\n    def encode(self, x):\n        x = self.resnet(x)  # ResNet\n        x = x.view(x.size(0), -1)  # flatten output of conv\n\n        # FC layers\n        x = self.bn1(self.fc1(x))\n        x = self.relu(x)\n        x = self.bn2(self.fc2(x))\n        x = self.relu(x)\n        # x = F.dropout(x, p=self.drop_p, training=self.training)\n        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def decode(self, z):\n        x = self.relu(self.fc_bn4(self.fc4(z)))\n        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n        x = self.convTrans6(x)\n        x = self.convTrans7(x)\n        x = self.convTrans8(x)\n        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n        return x\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_reconst = self.decode(z)\n\n        return x_reconst, z, mu, logvar","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:22:52.779421Z","iopub.execute_input":"2024-02-04T07:22:52.780194Z","iopub.status.idle":"2024-02-04T07:22:52.803748Z","shell.execute_reply.started":"2024-02-04T07:22:52.780163Z","shell.execute_reply":"2024-02-04T07:22:52.802694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_function(recon_x, x, mu, logvar):\n    MSE = F.mse_loss(recon_x, x, reduction='sum')\n#     MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return MSE + KLD\n\n# EncoderCNN architecture\nCNN_fc_hidden1, CNN_fc_hidden2 = 1024, 1024\nCNN_embed_dim = 256     # latent dim extracted by 2D CNN\nres_size = 224        # ResNet image size\ndropout_p = 0.2       # dropout probability\n\n\n# training parameters\nepochs = 100        # training epochs\nbatch_size = 64\nlearning_rate = 1e-4\n\nclass CustomDataset(Dataset):\n    def __init__(self, img_paths, transform=None):\n        self.img_paths = img_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = CustomDataset(data['img_path'].tolist(), transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:25:24.339018Z","iopub.execute_input":"2024-02-04T07:25:24.340015Z","iopub.status.idle":"2024-02-04T07:25:24.353908Z","shell.execute_reply.started":"2024-02-04T07:25:24.339958Z","shell.execute_reply":"2024-02-04T07:25:24.352997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\nmodel = ResNet_VAE(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop with tqdm\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    with tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\") as batch_bar:\n        for batch_idx, images in batch_bar:\n            images = images.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            recon_images, z, mu, logvar = model(images)\n            loss = loss_function(recon_images, images, mu, logvar)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()35\n            \n            # Update tqdm progress bar\n            batch_bar.set_postfix(loss=f\"{loss.item():.4f}\", epoch_loss=f\"{epoch_loss/(batch_idx + 1):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T07:25:24.985800Z","iopub.execute_input":"2024-02-04T07:25:24.986277Z","iopub.status.idle":"2024-02-04T08:43:04.254678Z","shell.execute_reply.started":"2024-02-04T07:25:24.986241Z","shell.execute_reply":"2024-02-04T08:43:04.253833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the image transformation\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n])\n\ndef denormalize(tensor):\n    \"\"\"\n    Denormalizes a tensor from the range [-1, 1] back to [0, 1]\n    \"\"\"\n    return tensor * 0.5 + 0.5\n\ndef generate_and_plot_image(image_path, model):\n    \"\"\"\n    Generates a reconstructed image using the provided VAE model and plots\n    it alongside the original image.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    \n    # Load and transform the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension\n    \n    # Move the tensor to the same device as the model\n    device = next(model.parameters()).device\n    image_tensor = image_tensor.to(device)\n    \n    # Generate the reconstructed image\n    with torch.no_grad():\n        reconstructed, _, _, _ = model(image_tensor)\n    \n    # Move tensors back to CPU for plotting\n    image_tensor = image_tensor.cpu()\n    reconstructed = reconstructed.cpu()\n    \n    # Denormalize the images for display\n    original_img = denormalize(image_tensor.squeeze()).permute(1, 2, 0).numpy()\n    reconstructed_img = denormalize(reconstructed.squeeze()).permute(1, 2, 0).numpy()\n    \n    # Plotting the original and reconstructed images\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(original_img)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    \n    axs[1].imshow(reconstructed_img)\n    axs[1].set_title('Reconstructed Image')\n    axs[1].axis('off')\n    \n    plt.show()\n\n# Example usage\n# Ensure 'vae' is your trained ResNet_VAE model and 'image_path' is a valid image file path\ngenerate_and_plot_image(data[\"img_path\"][0], model)  # Replace 'vae' with your model variable","metadata":{"execution":{"iopub.status.busy":"2024-02-04T08:44:03.521965Z","iopub.execute_input":"2024-02-04T08:44:03.522864Z","iopub.status.idle":"2024-02-04T08:44:03.948454Z","shell.execute_reply.started":"2024-02-04T08:44:03.522830Z","shell.execute_reply":"2024-02-04T08:44:03.947549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}